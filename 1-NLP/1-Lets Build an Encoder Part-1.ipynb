{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build an Encoder (Part 1): Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of the Full process\n",
    "- Given a Corpus of Text (List of Sentences)\n",
    "- **Normalize** each sentence\n",
    "  - Lower case\n",
    "  - Lemmatization & Stemming\n",
    "  - Remove symbols\n",
    "- Apply **Tokenization** on the normalized sentence\n",
    "- Build **Vocabulary** of such Tokens\n",
    "- **Encode** the tokens given their IDs\n",
    "- Apply **Padding** on each Sequence of IDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Figure For Encoder Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder-Pipeline](../imgs/Encoder-Pipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = PorterStemmer()\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Machine Can only understand digits.\",\n",
    "    \"Tokenizing Sentence into Sequence of Tokens\",\n",
    "    \"Encoding Tokens into Sequence of IDs\",\n",
    "    \"Apply Padding on each sequence\",\n",
    "    \"Pass sequences into our model\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence: str):\n",
    "    \"\"\"Apply Lower, Stemming\n",
    "\n",
    "    Args:\n",
    "        x (str): sentence\n",
    "        stemming (bool, optional): whether to apply stemming or not. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: list of normalized sentences\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(sentences: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Generate a vocabulary given corpus\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences (all data)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with key as the string (token) and value as the index\n",
    "        example\n",
    "        {\n",
    "            \"token-x\": 1, # let the 0 be for the padding token\n",
    "            \"token-y\": 2,\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    vocab = {...}\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(data_normalized: List[str], vocab: dict) -> List[int]:\n",
    "    \"\"\"Apply Encoding on the given list of sentences via the vocabulary\n",
    "\n",
    "    Args:\n",
    "        data_normalized (List[str]): List of normalized sentences to be encoded\n",
    "        vocab (dict): dictionary containing each token mapped to index\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of indexes (encoded tokens)\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(inp_seq: List[int], max_length: int) -> List[int]:\n",
    "    \"\"\"Apply padding given sequence of encoded tokens\n",
    "\n",
    "    Args:\n",
    "        inp_seq (List[int]): Sequence of tokens after encoding\n",
    "        max_length (int): length where all sequences will have\n",
    "\n",
    "    Returns:\n",
    "        list: list of encoded sequence with padding to be all the same size\n",
    "    \"\"\"\n",
    "    \n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in this cell avoid removing the output on the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['machine', 'can', 'only', 'understand', 'digits.'],\n",
       " ['tokenizing', 'sentence', 'into', 'sequence', 'of', 'token'],\n",
       " ['encoding', 'token', 'into', 'sequence', 'of', 'id'],\n",
       " ['apply', 'padding', 'on', 'each', 'sequence'],\n",
       " ['pas', 'sequence', 'into', 'our', 'model']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean and tokenize Corpus\n",
    "data_tokens = ...\n",
    "data_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in this cell avoid removing the output on the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 1,\n",
       " 'machine': 2,\n",
       " 'padding': 3,\n",
       " 'on': 4,\n",
       " 'model': 5,\n",
       " 'can': 6,\n",
       " 'into': 7,\n",
       " 'id': 8,\n",
       " 'of': 9,\n",
       " 'our': 10,\n",
       " 'apply': 11,\n",
       " 'pas': 12,\n",
       " 'understand': 13,\n",
       " 'token': 14,\n",
       " 'each': 15,\n",
       " 'sentence': 16,\n",
       " 'encoding': 17,\n",
       " 'tokenizing': 18,\n",
       " 'digits.': 19,\n",
       " 'only': 20}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Vocabulary\n",
    "vocab = ...\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in this cell avoid removing the output on the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6, 20, 13, 19],\n",
       " [18, 16, 7, 1, 9, 14],\n",
       " [17, 14, 7, 1, 9, 8],\n",
       " [11, 3, 4, 15, 1],\n",
       " [12, 1, 7, 10, 5]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = ...\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in this cell avoid removing the output on the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6, 20, 13, 19, 0],\n",
       " [18, 16, 7, 1, 9, 14],\n",
       " [17, 14, 7, 1, 9, 8],\n",
       " [11, 3, 4, 15, 1, 0],\n",
       " [12, 1, 7, 10, 5, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = ...\n",
    "data_tokens_padded = ...\n",
    "data_tokens_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All sequences having the same lenght\n",
    "assert all(list(map(len, data_tokens_padded)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Work! ðŸŽ‰  \n",
    "Now That you had learned the main concept of the Encoder, Let's work on a real world scenario and build a real tokenizer given an actual data of [Amazon Food reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews), which will have training and testing datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_img",
   "language": "python",
   "name": "ad_img"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173b1ad0e598a21817c913127c0ed612d69b6c3d0fd1e8e7f3c7ab65a8d7b09a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
